{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters**"
      ],
      "metadata": {
        "id": "ClC47lq54XjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from os import path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rV6J8Q_R2CMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "batch_size = 4\n",
        "margin = 1   # Margin for constrastive loss."
      ],
      "metadata": {
        "id": "mhT4YhFm4WhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuRQtc7Jk_m8",
        "outputId": "6ba5d5ae-7a65-491d-f6c9-46a5bae7898a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dataset_path = \"/content/drive/MyDrive/Sizvy_Towhid_Rigan/\"\n",
        "dataset_version = \"Final_Database/\"\n",
        "\n",
        "roi_folder_name = 'yolo_vein_roi'\n",
        "# roi_folder_name = 'ROI'\n",
        "\n",
        "number_of_roi_per_user = 5\n",
        "\n",
        "landmarks = [\"dorsal\", \"wrist\"]\n",
        "\n",
        "# ! ls /content/drive/MyDrive/vein_dataset/data_v1/"
      ],
      "metadata": {
        "id": "5GVAMVfhlvlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_path = base_dataset_path + dataset_version\n",
        "print(base_path)\n",
        "users = next(os.walk(base_path))[1]\n",
        "#sort users\n",
        "users.sort()\n",
        "\n",
        "# load a csv file\n",
        "df = pd.read_csv(\"/content/data_quality.csv\")\n",
        "\n",
        "\n",
        "very_good_difference = {\"dorsal\" : [], \"wrist\" : []}\n",
        "good_difference = {\"dorsal\" : [], \"wrist\" : []}\n",
        "average_difference = {\"dorsal\" : [], \"wrist\" : []}\n",
        "swapped_photo = {\"dorsal\": [], \"wrist\" : []}\n",
        "no_difference = {\"dorsal\": [], \"wrist\" : []}\n",
        "data_bad = {\"dorsal\": [], \"wrist\" : []}\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if \"a\" == row[\"dorsal\"]:\n",
        "        very_good_difference[\"dorsal\"].append(row['x'])\n",
        "    elif \"b\" == row['dorsal']:\n",
        "        good_difference[\"dorsal\"].append(row['x'])\n",
        "    elif \"c\" == row[\"dorsal\"]:\n",
        "        average_difference[\"dorsal\"].append(row['x'])\n",
        "    elif \"d\" == row[\"dorsal\"]:\n",
        "        swapped_photo[\"dorsal\"].append(row['x'])\n",
        "    elif \"e\" == row[\"dorsal\"]:\n",
        "        no_difference[\"dorsal\"].append(row['x'])\n",
        "    elif \"f\" == row[\"dorsal\"]:\n",
        "        data_bad[\"dorsal\"].append(row['x'])\n",
        "    else:\n",
        "        print(row['x'])\n",
        "\n",
        "    if \"a\" == row[\"wrist\"]:\n",
        "        very_good_difference[\"wrist\"].append(row['x'])\n",
        "    elif \"b\" == row['wrist']:\n",
        "        good_difference[\"wrist\"].append(row['x'])\n",
        "    elif \"c\" == row[\"wrist\"]:\n",
        "        average_difference[\"wrist\"].append(row['x'])\n",
        "    elif \"d\" == row[\"wrist\"]:\n",
        "        swapped_photo[\"wrist\"].append(row['x'])\n",
        "    elif \"e\" == row[\"wrist\"]:\n",
        "        no_difference[\"wrist\"].append(row['x'])\n",
        "    elif \"f\" == row[\"wrist\"]:\n",
        "        data_bad[\"wrist\"].append(row['x'])\n",
        "\n",
        "\n",
        "print(\"Dorsal\")\n",
        "print(\"Very good cnt: \", len(very_good_difference[\"dorsal\"]))\n",
        "print(\"Good cnt: \", len(good_difference[\"dorsal\"]))\n",
        "print(\"Average cnt: \", len(average_difference[\"dorsal\"]))\n",
        "print(\"Swapped cnt : \", len(swapped_photo[\"dorsal\"]))\n",
        "print(\"No difference cnt: \", len(no_difference[\"dorsal\"]))\n",
        "print(\"Data bad cnt: \", len(data_bad[\"dorsal\"]))\n",
        "\n",
        "\n",
        "total = len(very_good_difference[\"dorsal\"]) + len(good_difference[\"dorsal\"]) + len(average_difference[\"dorsal\"]) + len(swapped_photo[\"dorsal\"]) +  len(no_difference[\"dorsal\"]) + len(data_bad[\"dorsal\"])\n",
        "\n",
        "print(\"Dorsal total : \", total)\n",
        "\n",
        "\n",
        "print(\"Wrist\")\n",
        "print(\"Very good cnt: \", len(very_good_difference[\"wrist\"]))\n",
        "print(\"Good cnt: \", len(good_difference[\"wrist\"]))\n",
        "print(\"Average cnt: \", len(average_difference[\"wrist\"]))\n",
        "print(\"Swapped cnt : \", len(swapped_photo[\"wrist\"]))\n",
        "print(\"No difference cnt: \", len(no_difference[\"wrist\"]))\n",
        "print(\"Data bad cnt: \", len(data_bad[\"wrist\"]))\n",
        "\n",
        "total = len(very_good_difference[\"wrist\"]) + len(good_difference[\"wrist\"]) + len(average_difference[\"wrist\"]) + len(swapped_photo[\"wrist\"]) +  len(no_difference[\"wrist\"]) + len(data_bad[\"wrist\"])\n",
        "\n",
        "print(\"Wrist total : \", total)\n",
        "\n",
        "dorsal_good = very_good_difference[\"dorsal\"] + good_difference[\"dorsal\"] + swapped_photo[\"dorsal\"]\n",
        "dorsal_average = average_difference[\"dorsal\"]\n",
        "dorsal_good_average = dorsal_good + dorsal_average\n",
        "\n",
        "wrist_good = very_good_difference[\"wrist\"] + good_difference[\"wrist\"] + swapped_photo[\"wrist\"]\n",
        "wrist_average = average_difference[\"wrist\"]\n",
        "wrist_good_average = wrist_good + wrist_average\n",
        "\n",
        "# take all the intersection of dorsal_good_average and wrist_good_average\n",
        "#intersection = list(set(dorsal_good) & set(wrist_good))\n",
        "#intersection = dorsal_good\n",
        "intersection = []\n",
        "for name in users:\n",
        "  if name.endswith(\"_band\"):\n",
        "    intersection.append(name)\n",
        "# make intersection a set\n",
        "intersection = list(set(intersection))\n",
        "print(intersection)\n",
        "\n",
        "intersection = wrist_good\n",
        "\n",
        "remove_users = [\"sizvy\", \"mansur\", \"hasina\"]\n",
        "# remove  remove_users from intersection list if the user is present\n",
        "for user in remove_users:\n",
        "    if user in intersection:\n",
        "        intersection.remove(user)\n",
        "\n",
        "print(\"Intersection after delete: \", len(intersection))\n",
        "\n",
        "rantom_persons = random.sample(very_good_difference[\"dorsal\"], 5)\n",
        "\n",
        "#intersection = intersection + rantom_persons\n",
        "# shuffle the intersection list\n",
        "#intersection = random.sample(intersection, 25)\n",
        "intersection = random.sample(intersection, 40)\n",
        "\n",
        "\n",
        "print(intersection)\n",
        "print(\"Intersection: \", len(intersection))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "users = intersection\n",
        "length = len(users)\n",
        "print(length)\n",
        "training_len = (int)(length*0.9)\n",
        "users_train_val = users[:training_len]\n",
        "users_test = users[training_len:]\n",
        "\n",
        "print(users_train_val)\n",
        "print(users_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYvoKaR8o_FK",
        "outputId": "8363c4b9-26c2-4f93-f1e2-a4a428d43165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Sizvy_Towhid_Rigan/Final_Database/\n",
            "Dorsal\n",
            "Very good cnt:  20\n",
            "Good cnt:  25\n",
            "Average cnt:  17\n",
            "Swapped cnt :  19\n",
            "No difference cnt:  4\n",
            "Data bad cnt:  0\n",
            "Dorsal total :  85\n",
            "Wrist\n",
            "Very good cnt:  11\n",
            "Good cnt:  28\n",
            "Average cnt:  19\n",
            "Swapped cnt :  8\n",
            "No difference cnt:  17\n",
            "Data bad cnt:  2\n",
            "Wrist total :  85\n",
            "['dear_band', 'huda_band', 'abdullah_band', 'nakib_band', 'tamim_17_band', 'shakib_emn_band', 'sadat_band', 'emon_17_band', 'shakib_17_band', 'alamin_17_band', 'mosto_sharif_band', 'mehrab_band', 'rakib_band', 'sharif_17_band', 'asif18_band', 'nurul_band', 'tm_band', 'sakib_emn2_band', 'zakir_band', 'pappu_band']\n",
            "Intersection after delete:  44\n",
            "['per2', 'mujahid', 'shahan', 'per14', 'per1', 'tushar_5003', 'per84', 'rigan', 'per13', 'tm_band', 'per8', 'zahid', 'emon_5011', 'tanvir_5001', 'rakib_band', 'emon_5001', 'farsi1', 'fazle_rabbi_5011', 'towhid', 'shakib_emn_band', 'per5', 'dipto_17', 'nafiz', 'rabbi_5009', 'mahir_5003', 'tamim_17_band', 'sadat_band', 'shirin', 'per87', 'hasan_srbd', 'per15', 'monzur_srbd', 'zakir_band', 'pappu_band', 'per22', 'dear_band', 'per12', 'niaz', 'per89', 'junior']\n",
            "Intersection:  40\n",
            "40\n",
            "['per2', 'mujahid', 'shahan', 'per14', 'per1', 'tushar_5003', 'per84', 'rigan', 'per13', 'tm_band', 'per8', 'zahid', 'emon_5011', 'tanvir_5001', 'rakib_band', 'emon_5001', 'farsi1', 'fazle_rabbi_5011', 'towhid', 'shakib_emn_band', 'per5', 'dipto_17', 'nafiz', 'rabbi_5009', 'mahir_5003', 'tamim_17_band', 'sadat_band', 'shirin', 'per87', 'hasan_srbd', 'per15', 'monzur_srbd', 'zakir_band', 'pappu_band', 'per22', 'dear_band']\n",
            "['per12', 'niaz', 'per89', 'junior']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "def list_files_in_directory(directory):\n",
        "  files = []\n",
        "  for file in os.listdir(directory):\n",
        "    # Join the directory path with the file name to get the full path\n",
        "    full_path = os.path.join(directory, file)\n",
        "    if os.path.isfile(full_path):\n",
        "      files.append(full_path)\n",
        "  return files\n",
        "\n",
        "def get_file_path_list(dir):\n",
        "  # Get a list of all files in the current directory\n",
        "  all_files = list_files_in_directory(dir)\n",
        "  if len(all_files) < number_of_roi_per_user:\n",
        "    first_file = all_files[0]\n",
        "    while(len(all_files) < number_of_roi_per_user):\n",
        "      all_files.append(first_file)\n",
        "  # Get a list of all file paths in the current directory\n",
        "  all_file_paths = [os.path.join(dir, file) for file in all_files]\n",
        "\n",
        "\n",
        "  return all_file_paths[:number_of_roi_per_user]"
      ],
      "metadata": {
        "id": "GavZA-VYxQjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZmHGo76Bk78"
      },
      "outputs": [],
      "source": [
        "def create_triplets(landmark, hand, users):\n",
        "  triplets = pd.DataFrame(columns=['anchor_image', 'hydrated_image', 'dehydrated_image'])\n",
        "\n",
        "  index = 0\n",
        "\n",
        "  for user in users:\n",
        "    hydrated_list_url = base_dataset_path + dataset_version + user + \"/\" + \"hydrated/\" + hand + \"/\" + landmark + \"/\" + roi_folder_name\n",
        "    hydrated_list = get_file_path_list(hydrated_list_url)\n",
        "\n",
        "    #print(\"user: \", user)\n",
        "    #print(\"hydrated: \", len(hydrated_list))\n",
        "\n",
        "    dehydrated_list_url = base_dataset_path + dataset_version + user + \"/\" + \"dehydrated/\" + hand + \"/\" + landmark + \"/\" + roi_folder_name\n",
        "    dehydrated_list = get_file_path_list(dehydrated_list_url)\n",
        "\n",
        "    #print(\"dehydrated: \", len(dehydrated_list))\n",
        "    if ((len(hydrated_list) == 0) or (len(dehydrated_list) == 0)):\n",
        "      continue\n",
        "\n",
        "    anchor = hydrated_list[0]\n",
        "\n",
        "    for i in range(len(dehydrated_list)):\n",
        "      dehydrated = dehydrated_list[i]\n",
        "      for j in range(len(hydrated_list)):\n",
        "        hydrated = hydrated_list[j]\n",
        "        triplets.loc[len(triplets)] = [anchor, hydrated, dehydrated]\n",
        "\n",
        "    #print(\"length of triplets: \", len(triplets))\n",
        "  return triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtL8MghTKjOt"
      },
      "outputs": [],
      "source": [
        "def load_images(triplets):\n",
        "  anchor_images_array = []\n",
        "  hydrated_images_array = []\n",
        "  dehydrated_images_array = []\n",
        "\n",
        "  errorIndices = []\n",
        "  count = 0\n",
        "  fileError = 0\n",
        "\n",
        "  for i, directory in triplets['anchor_image'].items():\n",
        "    try:\n",
        "      img = image.load_img(directory, target_size=(200, 200))\n",
        "      img = image.img_to_array(img)\n",
        "      img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "      img = preprocess_input(img)\n",
        "\n",
        "      anchor_images_array.append(img)\n",
        "      count = count + 1\n",
        "    except FileNotFoundError:\n",
        "      fileError = fileError + 1\n",
        "\n",
        "  print(\"anchor image array: \", len(anchor_images_array))\n",
        "\n",
        "  for i, directory in triplets['hydrated_image'].items():\n",
        "    try:\n",
        "      img = image.load_img(directory, target_size=(200, 200))\n",
        "      img = image.img_to_array(img)\n",
        "      img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "      img = preprocess_input(img)\n",
        "\n",
        "      hydrated_images_array.append(img)\n",
        "      count = count + 1\n",
        "    except FileNotFoundError:\n",
        "      fileError = fileError + 1\n",
        "\n",
        "  print(\"hydrated image array: \", len(hydrated_images_array))\n",
        "\n",
        "  for i, directory in triplets['dehydrated_image'].items():\n",
        "    try:\n",
        "      img = image.load_img(directory, target_size=(200, 200))\n",
        "      img = image.img_to_array(img)\n",
        "      img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "      img = preprocess_input(img)\n",
        "\n",
        "      dehydrated_images_array.append(img)\n",
        "      count = count + 1\n",
        "    except FileNotFoundError:\n",
        "      fileError = fileError + 1\n",
        "\n",
        "  print(\"dehydrated image array : \", len(dehydrated_images_array))\n",
        "\n",
        "  image_triplets = []\n",
        "\n",
        "  for i in range(len(anchor_images_array)):\n",
        "    anchor_img = anchor_images_array[i]\n",
        "    hydrated_img = hydrated_images_array[i]\n",
        "    dehydrated_img = dehydrated_images_array[i]\n",
        "    image_triplets += [[anchor_img, hydrated_img, dehydrated_img]]\n",
        "\n",
        "  print(\"image triplets : \", len(image_triplets))\n",
        "  return image_triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr0s7o9tGpLe"
      },
      "outputs": [],
      "source": [
        "def make_pairs(triplets):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for idx in range(len(triplets)):\n",
        "      # add a matching example\n",
        "      x1 = triplets[idx][0]\n",
        "      x2 = triplets[idx][1]\n",
        "      pairs += [[x1, x2]]\n",
        "      labels += [1]\n",
        "\n",
        "      # add a non-matching example\n",
        "      x1 = triplets[idx][0]\n",
        "      x2 = triplets[idx][2]\n",
        "      pairs += [[x1, x2]]\n",
        "      labels += [0]\n",
        "\n",
        "    return np.array(pairs), np.array(labels).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYIndxzyODNs"
      },
      "outputs": [],
      "source": [
        "def split_dataset_pair(x, y):\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # index 0 has anchor that means x_train_1 has acnhor images only and x_train_2 hydrated or dehydarted\n",
        "  x_train_1 = x_train[:, 0]\n",
        "  x_train_2 = x_train[:, 1]\n",
        "\n",
        "  x_val_1 = x_val[:, 0]\n",
        "  x_val_2 = x_val[:, 1]\n",
        "\n",
        "  return (x_train_1, x_train_2, x_val_1, x_val_2, y_train, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE5KNHIs2DQl"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
        "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
        "\n",
        "def manhattan_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sum(K.abs(x - y), axis=1, keepdims=True)\n",
        "\n",
        "def cosine_distance(vects):\n",
        "    x, y = vects\n",
        "    x_norm = tf.nn.l2_normalize(x, axis = 1)\n",
        "    y_norm = tf.nn.l2_normalize(y, axis = 1)\n",
        "    cos = tf.math.reduce_sum(x_norm * y_norm, axis = 1, keepdims=True)\n",
        "    cosx = tf.clip_by_value(cos, -1.0, 1.0)\n",
        "    return tf.math.abs(cos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi Modal Siamese Model**"
      ],
      "metadata": {
        "id": "e81OMCE_6W-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding for dorsal"
      ],
      "metadata": {
        "id": "3qBWWWia6z1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = layers.Input((200, 200, 3))\n",
        "x = tf.keras.layers.BatchNormalization()(input)\n",
        "x = layers.Conv2D(4, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Conv2D(16, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = layers.Dense(10, activation=\"relu\")(x)\n",
        "embedding_network_dorsal_left = keras.Model(input, x)\n",
        "embedding_network_dorsal_right = keras.Model(input, x)"
      ],
      "metadata": {
        "id": "74Gyn3Fo6Un5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input2 = layers.Input((200, 200, 3))\n",
        "x2 = tf.keras.layers.BatchNormalization()(input2)\n",
        "x2 = layers.Conv2D(5, (5, 5), activation=\"relu\")(x2)\n",
        "x2 = layers.AveragePooling2D(pool_size=(2, 2))(x2)\n",
        "x2 = layers.Conv2D(20, (5, 5), activation=\"relu\")(x2)\n",
        "x2 = layers.AveragePooling2D(pool_size=(2, 2))(x2)\n",
        "x2 = layers.Flatten()(x2)\n",
        "\n",
        "x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "x2 = layers.Dense(10, activation=\"relu\")(x2)\n",
        "embedding_network_dorsal_right = keras.Model(input2, x2)"
      ],
      "metadata": {
        "id": "gpHOuoSvQ63r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding for wrist"
      ],
      "metadata": {
        "id": "lOFyAx4N64Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input = layers.Input((200, 200, 3))\n",
        "# x = tf.keras.layers.BatchNormalization()(input)\n",
        "# x = layers.Conv2D(5, (5, 5), activation=\"relu\")(x)\n",
        "# x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "# x = layers.Conv2D(25, (5, 5), activation=\"relu\")(x)\n",
        "# x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "# x = layers.Flatten()(x)\n",
        "\n",
        "# x = tf.keras.layers.BatchNormalization()(x)\n",
        "# x = layers.Dense(10, activation=\"relu\")(x)\n",
        "# embedding_network_wrist = keras.Model(input, x)"
      ],
      "metadata": {
        "id": "oE0YTuhz6688"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Siamese model only for dorsal\n",
        "*(Left hand + Right hand)* <br>\n",
        "ref = anchor (hydrated) <br>\n",
        "state = hydrated or dehydrated\n",
        "\n",
        "left hand dorsal from both tower (ref and state tower) share same embedding means they share weight. <br>\n",
        "same is for right hand dorsal"
      ],
      "metadata": {
        "id": "QI3nJH8R6-PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_left_dorsal_ref = layers.Input((200, 200, 3))\n",
        "input_left_dorsal_state = layers.Input((200, 200, 3))\n",
        "\n",
        "input_right_dorsal_ref = layers.Input((200, 200, 3))\n",
        "input_right_dorsal_state = layers.Input((200, 200, 3))\n",
        "\n",
        "tower_ref_left = embedding_network_dorsal_left(input_left_dorsal_ref)\n",
        "tower_state_left = embedding_network_dorsal_left(input_left_dorsal_state)\n",
        "\n",
        "tower_ref_right = embedding_network_dorsal_right(input_right_dorsal_ref)\n",
        "tower_state_right = embedding_network_dorsal_right(input_right_dorsal_state)\n",
        "\n",
        "# tower_ref feature merge\n",
        "# number row must be same as before but feature will be merged\n",
        "tower_ref = layers.Concatenate(axis=-1)([tower_ref_left, tower_ref_right])\n",
        "tower_state = layers.Concatenate(axis=-1)([tower_state_left, tower_state_right])\n",
        "\n",
        "merge_layer = layers.Lambda(euclidean_distance)([tower_ref, tower_state])\n",
        "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
        "output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
        "\n",
        "# order in inputs must be maintained\n",
        "siamese = keras.Model(inputs=[input_left_dorsal_ref, input_right_dorsal_ref, input_left_dorsal_state, input_right_dorsal_state], outputs=output_layer)"
      ],
      "metadata": {
        "id": "hIH-l0Xf69s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-jjwIwt2DQl"
      },
      "outputs": [],
      "source": [
        "def loss(margin=1):\n",
        "    def contrastive_loss(y_true, y_pred):\n",
        "        square_pred = tf.math.square(y_pred)\n",
        "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
        "        return tf.math.reduce_mean(\n",
        "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
        "        )\n",
        "\n",
        "    return contrastive_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
        "siamese.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Floop5Yk3gmk",
        "outputId": "ee7a0344-b2a2-4aa0-c68f-4778c28dc4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_27 (InputLayer)       [(None, 200, 200, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " input_29 (InputLayer)       [(None, 200, 200, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)       [(None, 200, 200, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)       [(None, 200, 200, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " model_16 (Functional)       (None, 10)                   496758    ['input_27[0][0]',            \n",
            "                                                                     'input_28[0][0]']            \n",
            "                                                                                                  \n",
            " model_18 (Functional)       (None, 10)                   621442    ['input_29[0][0]',            \n",
            "                                                                     'input_30[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate  (None, 20)                   0         ['model_16[0][0]',            \n",
            " )                                                                   'model_18[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate  (None, 20)                   0         ['model_16[1][0]',            \n",
            " )                                                                   'model_18[1][0]']            \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)           (None, 1)                    0         ['concatenate_8[0][0]',       \n",
            "                                                                     'concatenate_9[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, 1)                    4         ['lambda_4[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 1)                    2         ['batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1118206 (4.27 MB)\n",
            "Trainable params: 959144 (3.66 MB)\n",
            "Non-trainable params: 159062 (621.34 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load train data\n",
        "left_hand = \"left_hand\"\n",
        "right_hand = \"right_hand\"\n",
        "landmark = \"wrist\"\n",
        "\n",
        "triplets_left_hand = create_triplets(landmark, left_hand, users_train_val)\n",
        "triplets_left_hand = triplets_left_hand[triplets_left_hand.notna()]\n",
        "\n",
        "triplets_right_hand = create_triplets(landmark, right_hand, users_train_val)\n",
        "triplets_right_hand = triplets_right_hand[triplets_right_hand.notna()]\n",
        "\n",
        "image_triplets_left_hand = load_images(triplets_left_hand)\n",
        "image_triplets_right_hand = load_images(triplets_right_hand)\n",
        "\n",
        "res_left_hand = make_pairs(image_triplets_left_hand)\n",
        "res_right_hand = make_pairs(image_triplets_right_hand)\n",
        "\n",
        "# res has pairs and labels\n",
        "# pairs[index] = [x1, x2]\n",
        "# where x1 = anchor and x2 = hydrated or dehydrated\n",
        "# labels[index] = 0 if x2 dehydrated 1 if hydrated\n",
        "pairs_left_hand = res_left_hand[0]\n",
        "labels_left_hand = res_left_hand[1]\n",
        "\n",
        "pairs_right_hand = res_right_hand[0]\n",
        "labels_right_hand = res_right_hand[1]\n",
        "\n",
        "# indx 0, 2, 4, ... all even has hydrated for both hand\n",
        "# indx 1, 3, 5, ... all odd has dehydrated for both hand\n",
        "print(\"check label left right disim: \", len(labels_left_hand!=labels_right_hand))\n",
        "\n",
        "(x_train_left_dorsal_ref, x_train_left_dorsal_state, x_val_left_dorsal_ref,\n",
        "x_val_left_dorsal_state, y_train_left_dorsal, y_val_left_dorsal) = split_dataset_pair(pairs_left_hand, labels_left_hand)\n",
        "\n",
        "(x_train_right_dorsal_ref, x_train_right_dorsal_state, x_val_right_dorsal_ref,\n",
        "x_val_right_dorsal_state, y_train_right_dorsal, y_val_right_dorsal) = split_dataset_pair(pairs_right_hand, labels_right_hand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdu2OmFMrSx6",
        "outputId": "a7900e05-7221-4c47-fce3-7bf35c98eef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anchor image array:  900\n",
            "hydrated image array:  900\n",
            "dehydrated image array :  900\n",
            "image triplets :  900\n",
            "anchor image array:  900\n",
            "hydrated image array:  900\n",
            "dehydrated image array :  900\n",
            "image triplets :  900\n",
            "check label left right disim:  1800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load test data\n",
        "triplets_left_hand_test = create_triplets(landmark, left_hand, users_test)\n",
        "triplets_left_hand_test = triplets_left_hand_test[triplets_left_hand_test.notna()]\n",
        "\n",
        "triplets_right_hand_test = create_triplets(landmark, right_hand, users_test)\n",
        "triplets_right_hand_test = triplets_right_hand_test[triplets_right_hand_test.notna()]\n",
        "\n",
        "image_triplets_left_hand_test = load_images(triplets_left_hand_test)\n",
        "image_triplets_right_hand_test = load_images(triplets_right_hand_test)\n",
        "\n",
        "res_left_hand_test = make_pairs(image_triplets_left_hand_test)\n",
        "res_right_hand_test = make_pairs(image_triplets_right_hand_test)\n",
        "\n",
        "pairs_left_hand_test = res_left_hand_test[0]\n",
        "labels_left_hand_test = res_left_hand_test[1]\n",
        "\n",
        "pairs_right_hand_test = res_right_hand_test[0]\n",
        "labels_right_hand_test = res_right_hand_test[1]\n",
        "\n",
        "x_test_left_dorsal_ref = pairs_left_hand_test[:, 0]\n",
        "x_test_left_dorsal_state = pairs_left_hand_test[:, 1]\n",
        "\n",
        "x_test_right_dorsal_ref = pairs_right_hand_test[:, 0]\n",
        "x_test_right_dorsal_state = pairs_right_hand_test[:, 1]"
      ],
      "metadata": {
        "id": "5lKn27wqr4A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vy2mHEY31gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjn6G1B2o3t-"
      },
      "outputs": [],
      "source": [
        "def landmark_model(landmark):\n",
        "  history = siamese.fit(\n",
        "    [x_train_left_dorsal_ref, x_train_right_dorsal_ref, x_train_left_dorsal_state, x_train_right_dorsal_state],\n",
        "    y_train_left_dorsal,\n",
        "    validation_data=([x_val_left_dorsal_ref, x_val_right_dorsal_ref, x_val_left_dorsal_state, x_val_right_dorsal_state],\n",
        "                     y_val_left_dorsal),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "  )\n",
        "\n",
        "  predictions = siamese.predict([x_test_left_dorsal_ref, x_test_right_dorsal_ref, x_test_left_dorsal_state, x_test_right_dorsal_state])\n",
        "\n",
        "  return history, predictions, labels_left_hand_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_dorsal, predictions_dorsal, y_test_dorsal = landmark_model('wrist')"
      ],
      "metadata": {
        "id": "DLHFBUVf3uGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualize**"
      ],
      "metadata": {
        "id": "Czis8bFAt4kl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INyhN5mb2DQn"
      },
      "outputs": [],
      "source": [
        "def plt_metric(history, metric, title, has_valid=True):\n",
        "    plt.plot(history[metric])\n",
        "    if has_valid:\n",
        "        plt.plot(history[\"val_\" + metric])\n",
        "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.title(title)\n",
        "    plt.ylabel(metric)\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy\n",
        "plt_metric(history=history_dorsal.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
        "\n",
        "# Plot the constrastive loss\n",
        "plt_metric(history=history_dorsal.history, metric=\"loss\", title=\"Constrastive Loss\")"
      ],
      "metadata": {
        "id": "B57VSd_LuL7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate test accuracy\n",
        "test_accuracy = np.sum(np.round(predictions_dorsal) == y_test_dorsal) / len(y_test_dorsal)\n",
        "print(\"Test accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "HDVVzH2Q30fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for i in range(len(predictions_dorsal)):\n",
        "  if predictions_dorsal[i] >= 0.5:\n",
        "    predictions_dorsal[i] = 1\n",
        "  else:\n",
        "    predictions_dorsal[i] = 0\n",
        "  if predictions_dorsal[i] == y_test_dorsal[i]:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print(\"Accuracy: \", correct/total)"
      ],
      "metadata": {
        "id": "Vd6M97TOjrJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}